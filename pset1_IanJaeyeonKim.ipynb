{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce0fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy.stats import truncnorm\n",
    "from pandas import DataFrame as df\n",
    "import scipy.integrate as integrate\n",
    "from scipy.optimize import minimize     # GMM optimization\n",
    "from scipy.misc import derivative      # Jacobian Estimation\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81973bce",
   "metadata": {},
   "source": [
    "# Part 0: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615f08e",
   "metadata": {},
   "source": [
    "Before jumping into the main questions, in this part I will modify the given dataset to go through the given analysis. \n",
    "Structure of the given dataset requires imputation procedure for the following two reasons.  \n",
    "\n",
    "<b>1) To aggregate the dataset with two different regimes </b>  \n",
    "Throughout the problem set, we will assume that the random sample, log income level, follows the normal distribution. \n",
    "\n",
    "$$upto\\,panel\\, year\\, 2009 $$\n",
    "<img src=\"https://github.com/Ian-JY-Kim/Metrics/blob/main/pre_period.png?raw=true\" width = \"400\" height =\"300\"></img>\n",
    "Above graph shows the naive shape of the income distribution upto panel year 2009, where I used the bins rather than logged income level. I can check that the assumption of normal distribution is plausible. Since after transforming the income level with log function, the shape skewed to the right will come down.  \n",
    "\n",
    "$$from\\,panel\\, year\\, 2010 $$\n",
    "<img src=\"https://github.com/Ian-JY-Kim/Metrics/blob/main/post_period.png?raw=true\" width = \"400\" height =\"300\"></img> However, assuming the normal distribution with the post regime dataset seems not plausible. It is because the data shows us a highly skewed distribution resembles a pareto distribution rather than a normal distribution. This is due to the fact that after panel year 2010, high earners' income data is top coded at the bin 27, rather than 30\n",
    "\n",
    "To aggregate the the data with two different regime, I will estimate the first and second moment of the normal distribution using the pre regime data (upto panel year 2009). And by assuming the normal distribution with the estimated parameters, I will calculate the probability of one's income goes into each bin between 27 and 30 conditional on one's income falls above bin 27. Using the calculated probability, I will impute the income bin 27, 28, 29, 30 of the post regime dataset. And then, I will aggregate the dataset\n",
    "\n",
    "\n",
    "<b>2) To deal with the top coded data, especially with the top earners in bin 30 </b>  \n",
    "Apart from the aggregation issue, to deal with the three different assumption which are  \n",
    "(a) Assuming each household earns the lowest income in each bin  \n",
    "(b) Assuming each household earns the highes income in each bin  \n",
    "(c) Assuming each household earns the mean income in each bin  \n",
    "in question 7, I will go through another imputation procedure. Because, without imputation, we can not work with assumption (b) and (c).\n",
    "\n",
    "Since we already aggregated our dataset above, we don't have any other dataset to retrieve the first and second moment of the distribution from. Therefore, I will estimate the first and second moment using the data in bins from 1 to 29.  To estimate the moments, I will use the mean income level ($= \\frac{highest+lowest}{2}$) for each bin for the assumption (b) and the highest income value for the assumption (c). \n",
    "\n",
    "And from there, for the assumption (b), I will calculate the mean value of bin 30.  \n",
    "And for the assumption (c), I will draw 354410 many random sample from the truncated normal distribution with the range of bin 30 to find the maximum value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd54fcd",
   "metadata": {},
   "source": [
    "## Aggregate the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4148f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"simulated_income_data.csv\")\n",
    "dataset_pre = dataset[dataset['panel_year'] <= 2009]\n",
    "dataset_post = dataset[dataset['panel_year'] > 2009]\n",
    "dataset_pre = df(dataset_pre.groupby('household_income')['projection_factor'].sum())\n",
    "dataset_pre = dataset_pre.reset_index()\n",
    "\n",
    "income_amount = [0, 5000, 8000, 10000, 12000, 15000, 20000, 25000, 30000, 35000, 40000, \n",
    "                45000, 50000, 60000, 70000, 100000, 125000, 150000, 200000]\n",
    "dataset_pre['income_amount'] = income_amount \n",
    "total_projection = np.array(dataset_pre['projection_factor']).sum()\n",
    "dataset_pre['weight'] = dataset_pre['projection_factor']/total_projection\n",
    "log_income_amount = []\n",
    "for i in dataset_pre['income_amount']:\n",
    "    if i == 0:\n",
    "        log_income_amount.append(0)\n",
    "    else:\n",
    "        log_income_amount.append(np.log(i))\n",
    "dataset_pre['log_income_amount'] = log_income_amount\n",
    "mean_hat = np.inner(np.array(dataset_pre['weight']), np.array(dataset_pre['log_income_amount']))\n",
    "sigma_sq_hat = np.inner(np.array(dataset_pre['weight']), (np.array(dataset_pre['log_income_amount'])-mean_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf3d2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>household_income</th>\n",
       "      <th>projection_total</th>\n",
       "      <th>weight</th>\n",
       "      <th>income_amount</th>\n",
       "      <th>log_income_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>46530</td>\n",
       "      <td>0.013407</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>38061</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>5000</td>\n",
       "      <td>8.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>50001</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>8000</td>\n",
       "      <td>8.987197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>42577</td>\n",
       "      <td>0.012268</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.210340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>56830</td>\n",
       "      <td>0.016374</td>\n",
       "      <td>12000</td>\n",
       "      <td>9.392662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>127104</td>\n",
       "      <td>0.036622</td>\n",
       "      <td>15000</td>\n",
       "      <td>9.615805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>193586</td>\n",
       "      <td>0.055778</td>\n",
       "      <td>20000</td>\n",
       "      <td>9.903488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>93956</td>\n",
       "      <td>0.027072</td>\n",
       "      <td>25000</td>\n",
       "      <td>10.126631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>158063</td>\n",
       "      <td>0.045543</td>\n",
       "      <td>30000</td>\n",
       "      <td>10.308953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>188202</td>\n",
       "      <td>0.054227</td>\n",
       "      <td>35000</td>\n",
       "      <td>10.463103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18</td>\n",
       "      <td>137840</td>\n",
       "      <td>0.039716</td>\n",
       "      <td>40000</td>\n",
       "      <td>10.596635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19</td>\n",
       "      <td>109638</td>\n",
       "      <td>0.031590</td>\n",
       "      <td>45000</td>\n",
       "      <td>10.714418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21</td>\n",
       "      <td>297270</td>\n",
       "      <td>0.085652</td>\n",
       "      <td>50000</td>\n",
       "      <td>10.819778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23</td>\n",
       "      <td>221302</td>\n",
       "      <td>0.063764</td>\n",
       "      <td>60000</td>\n",
       "      <td>11.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26</td>\n",
       "      <td>652056</td>\n",
       "      <td>0.187877</td>\n",
       "      <td>70000</td>\n",
       "      <td>11.156251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27</td>\n",
       "      <td>406147</td>\n",
       "      <td>0.117023</td>\n",
       "      <td>100000</td>\n",
       "      <td>11.512925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28</td>\n",
       "      <td>132626</td>\n",
       "      <td>0.038214</td>\n",
       "      <td>125000</td>\n",
       "      <td>11.736069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>29</td>\n",
       "      <td>164459</td>\n",
       "      <td>0.047386</td>\n",
       "      <td>150000</td>\n",
       "      <td>11.918391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>354410</td>\n",
       "      <td>0.102116</td>\n",
       "      <td>200000</td>\n",
       "      <td>12.206073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    household_income  projection_total    weight  income_amount  \\\n",
       "0                  3             46530  0.013407              0   \n",
       "1                  4             38061  0.010967           5000   \n",
       "2                  6             50001  0.014407           8000   \n",
       "3                  8             42577  0.012268          10000   \n",
       "4                 10             56830  0.016374          12000   \n",
       "5                 11            127104  0.036622          15000   \n",
       "6                 13            193586  0.055778          20000   \n",
       "7                 15             93956  0.027072          25000   \n",
       "8                 16            158063  0.045543          30000   \n",
       "9                 17            188202  0.054227          35000   \n",
       "10                18            137840  0.039716          40000   \n",
       "11                19            109638  0.031590          45000   \n",
       "12                21            297270  0.085652          50000   \n",
       "13                23            221302  0.063764          60000   \n",
       "14                26            652056  0.187877          70000   \n",
       "15                27            406147  0.117023         100000   \n",
       "16                28            132626  0.038214         125000   \n",
       "17                29            164459  0.047386         150000   \n",
       "18                30            354410  0.102116         200000   \n",
       "\n",
       "    log_income_amount  \n",
       "0            0.000000  \n",
       "1            8.517193  \n",
       "2            8.987197  \n",
       "3            9.210340  \n",
       "4            9.392662  \n",
       "5            9.615805  \n",
       "6            9.903488  \n",
       "7           10.126631  \n",
       "8           10.308953  \n",
       "9           10.463103  \n",
       "10          10.596635  \n",
       "11          10.714418  \n",
       "12          10.819778  \n",
       "13          11.002100  \n",
       "14          11.156251  \n",
       "15          11.512925  \n",
       "16          11.736069  \n",
       "17          11.918391  \n",
       "18          12.206073  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm_pdf(beta):\n",
    "    return scipy.stats.norm(mean_hat,sigma_sq_hat**(1/2)).pdf(beta)\n",
    "dataset_post = df(dataset_post.groupby('household_income')['projection_factor'].sum())\n",
    "dataset_post = dataset_post.reset_index()\n",
    "\n",
    "prob_above27 = integrate.quad(norm_pdf, np.log(100000), 100)[0]\n",
    "prob_27 = integrate.quad(norm_pdf, np.log(100000), np.log(125000))[0] / prob_above27\n",
    "prob_28 = integrate.quad(norm_pdf, np.log(125000), np.log(150000))[0] / prob_above27\n",
    "prob_29 = integrate.quad(norm_pdf, np.log(150000), np.log(200000))[0] / prob_above27\n",
    "prob_30 = integrate.quad(norm_pdf, np.log(200000), 100)[0] / prob_above27\n",
    "\n",
    "imputed_27 = int(627298 * prob_27)\n",
    "imputed_28 = int(627298 * prob_28)\n",
    "imputed_29 = int(627298 * prob_29)\n",
    "imputed_30 = int(627298 * prob_30)\n",
    "\n",
    "household_income = list(dataset_pre['household_income'])\n",
    "imputed_list = list(dataset_post['projection_factor'])[:-1] + [imputed_27, imputed_28, imputed_29, imputed_30]\n",
    "imputed_dataset_post = df({'household_income': household_income, 'projection_factor_post': imputed_list})\n",
    "master = dataset_pre.merge(imputed_dataset_post)\n",
    "master['projection_total'] = master['projection_factor'] + master['projection_factor_post']\n",
    "master['weight'] = master['projection_total']/np.array(master['projection_total']).sum()\n",
    "aggregated_dataset = master[['household_income', 'projection_total','weight', 'income_amount', 'log_income_amount']]\n",
    "aggregated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca37e89",
   "metadata": {},
   "source": [
    "## Impute the Mean and Maximum Income of Bin 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76508e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_upto29 = aggregated_dataset[aggregated_dataset['household_income'] <= 29]\n",
    "income_mean = [2500, 6500, 9000, 11000, 13000, 17500, 22500, 27500, 32500, 37500, 42500, 47500, 55000, 65000,\n",
    "              85000, 112500, 137500, 175000]\n",
    "income_max = [5000, 8000, 10000, 12000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 60000, 70000, 100000,\n",
    "             125000, 150000, 200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235bfb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_upto29['income_amount_mean'] = income_mean\n",
    "data_upto29['income_amount_highest'] = income_max\n",
    "data_upto29['log_income_mean'] = [np.log(i) for i in data_upto29['income_amount_mean']]\n",
    "data_upto29['log_income_highest'] = [np.log(i) for i in data_upto29['income_amount_highest']]\n",
    "data_upto29['weight_upto29'] = data_upto29['projection_total']/np.array(data_upto29['projection_total']).sum()\n",
    "\n",
    "mean_hat_ass_b = np.inner(np.array(data_upto29['weight_upto29']), np.array(data_upto29['log_income_mean']))\n",
    "sigma_sq_hat_ass_b = np.inner(np.array(data_upto29['weight_upto29']), (np.array(data_upto29['log_income_mean'])-mean_hat_ass_b)**2)\n",
    "\n",
    "mean_hat_ass_c = np.inner(np.array(data_upto29['weight_upto29']), np.array(data_upto29['log_income_highest']))\n",
    "sigma_sq_hat_ass_c = np.inner(np.array(data_upto29['weight_upto29']), (np.array(data_upto29['log_income_highest'])-mean_hat_ass_b)**2)\n",
    "\n",
    "norm_pdf_mean = lambda x: scipy.stats.norm(mean_hat_ass_b,sigma_sq_hat_ass_b**(1/2)).pdf(x)\n",
    "condition = integrate.quad(norm_pdf_mean, np.log(200000), 100)[0]\n",
    "f = lambda x: x * (scipy.stats.norm(mean_hat_ass_b,sigma_sq_hat_ass_b**(1/2)).pdf(x)) / condition \n",
    "mean_value_bin30 = integrate.quad(f, np.log(200000), 100)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615599af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>household_income</th>\n",
       "      <th>projection_total</th>\n",
       "      <th>weight</th>\n",
       "      <th>income_amount</th>\n",
       "      <th>log_income_amount</th>\n",
       "      <th>log_income_mean</th>\n",
       "      <th>log_income_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>46530</td>\n",
       "      <td>0.013407</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.824046</td>\n",
       "      <td>8.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>38061</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>5000</td>\n",
       "      <td>8.517193</td>\n",
       "      <td>8.779557</td>\n",
       "      <td>8.987197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>50001</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>8000</td>\n",
       "      <td>8.987197</td>\n",
       "      <td>9.104980</td>\n",
       "      <td>9.210340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>42577</td>\n",
       "      <td>0.012268</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.210340</td>\n",
       "      <td>9.305651</td>\n",
       "      <td>9.392662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>56830</td>\n",
       "      <td>0.016374</td>\n",
       "      <td>12000</td>\n",
       "      <td>9.392662</td>\n",
       "      <td>9.472705</td>\n",
       "      <td>9.615805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>127104</td>\n",
       "      <td>0.036622</td>\n",
       "      <td>15000</td>\n",
       "      <td>9.615805</td>\n",
       "      <td>9.769956</td>\n",
       "      <td>9.903488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>193586</td>\n",
       "      <td>0.055778</td>\n",
       "      <td>20000</td>\n",
       "      <td>9.903488</td>\n",
       "      <td>10.021271</td>\n",
       "      <td>10.126631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>93956</td>\n",
       "      <td>0.027072</td>\n",
       "      <td>25000</td>\n",
       "      <td>10.126631</td>\n",
       "      <td>10.221941</td>\n",
       "      <td>10.308953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>158063</td>\n",
       "      <td>0.045543</td>\n",
       "      <td>30000</td>\n",
       "      <td>10.308953</td>\n",
       "      <td>10.388995</td>\n",
       "      <td>10.463103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>188202</td>\n",
       "      <td>0.054227</td>\n",
       "      <td>35000</td>\n",
       "      <td>10.463103</td>\n",
       "      <td>10.532096</td>\n",
       "      <td>10.596635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18</td>\n",
       "      <td>137840</td>\n",
       "      <td>0.039716</td>\n",
       "      <td>40000</td>\n",
       "      <td>10.596635</td>\n",
       "      <td>10.657259</td>\n",
       "      <td>10.714418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19</td>\n",
       "      <td>109638</td>\n",
       "      <td>0.031590</td>\n",
       "      <td>45000</td>\n",
       "      <td>10.714418</td>\n",
       "      <td>10.768485</td>\n",
       "      <td>10.819778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21</td>\n",
       "      <td>297270</td>\n",
       "      <td>0.085652</td>\n",
       "      <td>50000</td>\n",
       "      <td>10.819778</td>\n",
       "      <td>10.915088</td>\n",
       "      <td>11.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23</td>\n",
       "      <td>221302</td>\n",
       "      <td>0.063764</td>\n",
       "      <td>60000</td>\n",
       "      <td>11.002100</td>\n",
       "      <td>11.082143</td>\n",
       "      <td>11.156251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26</td>\n",
       "      <td>652056</td>\n",
       "      <td>0.187877</td>\n",
       "      <td>70000</td>\n",
       "      <td>11.156251</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>11.512925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27</td>\n",
       "      <td>406147</td>\n",
       "      <td>0.117023</td>\n",
       "      <td>100000</td>\n",
       "      <td>11.512925</td>\n",
       "      <td>11.630709</td>\n",
       "      <td>11.736069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28</td>\n",
       "      <td>132626</td>\n",
       "      <td>0.038214</td>\n",
       "      <td>125000</td>\n",
       "      <td>11.736069</td>\n",
       "      <td>11.831379</td>\n",
       "      <td>11.918391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>29</td>\n",
       "      <td>164459</td>\n",
       "      <td>0.047386</td>\n",
       "      <td>150000</td>\n",
       "      <td>11.918391</td>\n",
       "      <td>12.072541</td>\n",
       "      <td>12.206073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>354410</td>\n",
       "      <td>0.102116</td>\n",
       "      <td>200000</td>\n",
       "      <td>12.206073</td>\n",
       "      <td>12.554362</td>\n",
       "      <td>13.046157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    household_income  projection_total    weight  income_amount  \\\n",
       "0                  3             46530  0.013407              0   \n",
       "1                  4             38061  0.010967           5000   \n",
       "2                  6             50001  0.014407           8000   \n",
       "3                  8             42577  0.012268          10000   \n",
       "4                 10             56830  0.016374          12000   \n",
       "5                 11            127104  0.036622          15000   \n",
       "6                 13            193586  0.055778          20000   \n",
       "7                 15             93956  0.027072          25000   \n",
       "8                 16            158063  0.045543          30000   \n",
       "9                 17            188202  0.054227          35000   \n",
       "10                18            137840  0.039716          40000   \n",
       "11                19            109638  0.031590          45000   \n",
       "12                21            297270  0.085652          50000   \n",
       "13                23            221302  0.063764          60000   \n",
       "14                26            652056  0.187877          70000   \n",
       "15                27            406147  0.117023         100000   \n",
       "16                28            132626  0.038214         125000   \n",
       "17                29            164459  0.047386         150000   \n",
       "18                30            354410  0.102116         200000   \n",
       "\n",
       "    log_income_amount  log_income_mean  log_income_max  \n",
       "0            0.000000         7.824046        8.517193  \n",
       "1            8.517193         8.779557        8.987197  \n",
       "2            8.987197         9.104980        9.210340  \n",
       "3            9.210340         9.305651        9.392662  \n",
       "4            9.392662         9.472705        9.615805  \n",
       "5            9.615805         9.769956        9.903488  \n",
       "6            9.903488        10.021271       10.126631  \n",
       "7           10.126631        10.221941       10.308953  \n",
       "8           10.308953        10.388995       10.463103  \n",
       "9           10.463103        10.532096       10.596635  \n",
       "10          10.596635        10.657259       10.714418  \n",
       "11          10.714418        10.768485       10.819778  \n",
       "12          10.819778        10.915088       11.002100  \n",
       "13          11.002100        11.082143       11.156251  \n",
       "14          11.156251        11.350407       11.512925  \n",
       "15          11.512925        11.630709       11.736069  \n",
       "16          11.736069        11.831379       11.918391  \n",
       "17          11.918391        12.072541       12.206073  \n",
       "18          12.206073        12.554362       13.046157  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_truncated_normal(mean, sd, low, upp):\n",
    "    return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "X = get_truncated_normal(mean_hat_ass_c, sigma_sq_hat_ass_c**1/2, np.log(200000), upp=100)\n",
    "\n",
    "candidates = []\n",
    "for i in range(100):\n",
    "    random_draw = X.rvs(354410)\n",
    "    candidates.append(random_draw.max())\n",
    "\n",
    "max_value_bin30 = np.array(candidates).mean()\n",
    "\n",
    "income_mean30 = [np.log(i) for i in income_mean] + [mean_value_bin30]\n",
    "income_max30 = [np.log(i) for i in income_max] + [max_value_bin30]\n",
    "aggregated_dataset['log_income_mean'] = income_mean30\n",
    "aggregated_dataset['log_income_max'] = income_max30\n",
    "aggregated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a897b",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a86078",
   "metadata": {},
   "source": [
    "# Part 1: MoM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894cd8ff",
   "metadata": {},
   "source": [
    "## Q1. Log Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73764a6e",
   "metadata": {},
   "source": [
    "Let a log normal distribution is parametrized with $\\mu$ and $\\sigma$  \n",
    "\n",
    "Then the first moment of log normal distribution becomes $ exp[\\mu + \\frac{\\sigma^2}{2}]$ and the second moment becomes $(exp[\\sigma^2]-1) exp[2\\mu + \\sigma^2]$  \n",
    "\n",
    "Now, let's define the sample analogy of the first two moments with $\\overline{Y_n} := \\frac{1}{n}\\sum_i^n y_i$ and $S^2 := \\frac{1}{n-1}\\sum_i^n(y_i - \\overline{Y_n})^2$ \n",
    "\n",
    "Then, we have $S^2 = (exp[\\hat{\\sigma}^2]-1)\\overline{Y_n}^2$, From which, we get $\\hat{\\sigma}^2 = ln(\\frac{S^2}{\\overline{Y_n}^2} + 1)$  \n",
    "\n",
    "Then, from the $\\overline{Y_n} = exp[\\hat{\\mu} + \\frac{\\hat{\\sigma}^2}{2}]$ We get $\\hat{\\mu} = ln\\overline{Y_n} - \\frac{1}{2}ln(\\frac{S^2}{\\overline{Y_n}^2} + 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a130e65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated mean:  10.998012204189621\n",
      "estimated variance:  0.43773086897760277\n"
     ]
    }
   ],
   "source": [
    "sample_mean = np.inner(np.array(aggregated_dataset['weight']), np.array(aggregated_dataset['income_amount']))\n",
    "temp1 = (np.array(aggregated_dataset['income_amount']) - sample_mean)**2\n",
    "sample_variance = np.inner(np.array(aggregated_dataset['weight']), temp1)\n",
    "sigma_sq_hat = np.log(sample_variance/(sample_mean**2) + 1)\n",
    "mu_hat = np.log(sample_mean) - sigma_sq_hat/2\n",
    "\n",
    "print(\"estimated mean: \", mu_hat)\n",
    "print(\"estimated variance: \", sigma_sq_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f10b3",
   "metadata": {},
   "source": [
    "## Q2. Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5094218",
   "metadata": {},
   "source": [
    "under normal distribution, we can estimate first and second moment with its sample analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a1eb6cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated mean:  10.79578469735779\n",
      "estimated variance:  1.496574725684537\n"
     ]
    }
   ],
   "source": [
    "MoM_mu = np.inner(np.array(aggregated_dataset['weight']), np.array(aggregated_dataset['log_income_amount']))\n",
    "temp2 = (np.array(aggregated_dataset['log_income_amount']) - mu_hat2)**2\n",
    "MoM_sigma = np.inner(np.array(aggregated_dataset['weight']), temp2)**(1/2)\n",
    "\n",
    "print(\"estimated mean: \", MoM_mu)\n",
    "print(\"estimated variance: \", MoM_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c6f50",
   "metadata": {},
   "source": [
    "## Q3. Standard Error working with $lny_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65d350",
   "metadata": {},
   "source": [
    "<b>Mean Estimator</b>  \n",
    "\n",
    "under a normal distribution assumption, sample analogies become the moment estimators for the first and second moment.  \n",
    "Then, we can derive variance of mean estimator as follows,  \n",
    "\n",
    "$$Var(\\overline{Y_n}) = Var(\\frac{1}{n}\\sum_i^n\\ln y_i)= \\frac{1}{n^2}Var(\\sum_i^n\\ln y_i) = \\frac{1}{n^2}nVar(\\ln y_i) = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "Hence by pluggin in the $S^2$ into $\\sigma^2$, we can get an estimate for the standard error for the mean estimator, which is $\\frac{S}{\\sqrt{n}}$ \n",
    "\n",
    "\n",
    "<b>Variance Estimator</b>  \n",
    "\n",
    "Note that $\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi_{n-1}^2$ \n",
    "\n",
    "Since the chi-squared distribution with n-1 degrees of freedom has a variance of $2(n-1)$, the variance of variance estimator becomes  \n",
    "\n",
    "$$Var(S^2) = Var(\\frac{\\sigma^2 \\chi_{n-1}^2}{n-1})= \\frac{2\\sigma^4}{n-1}$$\n",
    "\n",
    "Hence by plugging in the $S^2$ into $\\sigma^2$, we get an estimate for the standard error for the variance estimator, which is $S^2\\sqrt{\\frac{2}{n-1}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b617df54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated standard error of MoM mean esimator:  0.0008033272450482495\n",
      "estimated standard error of MoM variance esimator:  0.0017002232994797742\n"
     ]
    }
   ],
   "source": [
    "n = np.array(aggregated_dataset['projection_total']).sum()\n",
    "MoM_std_err_mean = (sigmasq_hat2/n)**(1/2)\n",
    "MoM_std_err_var = sigmasq_hat2 * (2/(n-1))**(1/2)\n",
    "\n",
    "print(\"estimated standard error of MoM mean esimator: \", MoM_std_err_mean)\n",
    "print(\"estimated standard error of MoM variance esimator: \", MoM_std_err_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393b1fa",
   "metadata": {},
   "source": [
    "# Part 2: MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eab1cf",
   "metadata": {},
   "source": [
    "## Q.4~Q.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04892f5d",
   "metadata": {},
   "source": [
    "Due to the invariance property of MLE, we may concentrate on the logged income data and work with the normal distribution.  \n",
    "\n",
    "I will the aggregated data I made above. And I will calculate the log-likelihood using the weight value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc9f78",
   "metadata": {},
   "source": [
    "## q4.  \n",
    "$pr(\\ln y_1, \\cdots, \\ln{y_2} | \\mu, \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sigma \\sqrt{2\\pi}} exp[-\\frac{1}{2}(\\frac{\\ln y_i-\\mu}{\\sigma})^2]$\n",
    "\n",
    "\n",
    "## q5.\n",
    "$\\mathcal{L}(\\mu, \\sigma ; \\ln y_1, \\cdots, \\ln y_n) = -n \\ln (\\sigma \\sqrt{2\\pi}) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(\\ln y_i - \\mu)^2 = -\\frac{1}{2}n\\ln 2\\pi - \\frac{1}{2}n\\ln{\\sigma^2}- \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(\\ln y_i - \\mu)^2$\n",
    "\n",
    "## q6.\n",
    "score for $\\mu$ (and $\\sigma^2$) is a partial derivative of the log-likelihood w.r.t $\\mu$ (and $\\sigma^2$)  \n",
    "hence score for $\\mu = \\frac{\\partial \\mathcal{L}}{\\partial \\mu} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(\\ln y_i - \\mu)$  \n",
    "and score for $\\sigma^2 = \\frac{\\partial \\mathcal{L}}{\\partial \\sigma^2} = -\\frac{n}{2}  \\frac{1}{\\sigma^2} + \\frac{1}{2} \\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}(\\ln y_i - \\mu)^2$  \n",
    "hence the score vector, which is an overall gradient becomes $s = \\begin{bmatrix} \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(\\ln y_i - \\mu)\\\\-\\frac{n}{2} \\frac{1}{\\sigma^2} + \\frac{1}{2} \\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}(\\ln y_i - \\mu)^2 \\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b1441",
   "metadata": {},
   "source": [
    "## Q.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ee4de",
   "metadata": {},
   "source": [
    "Recall the first order derivative w.r.t each parameter:  \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(\\ln y_i - \\mu)\\\\\\  \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\sigma^2} = -\\frac{n}{2}  \\frac{1}{\\sigma^2} + \\frac{1}{2} \\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}(\\ln y_i - \\mu)^2$$  \n",
    "\n",
    "Under $\\widehat{\\mu}_{MLE}, \\widehat{\\sigma^2}_{MLE}$ we require\n",
    "$$\\frac{1}{\\widehat{\\sigma^2}_{MLE}}\\sum_{i=1}^{n}(\\ln y_i - \\widehat{\\mu}_{MLE}) = 0 \\\\\n",
    "-\\frac{n}{2}  \\frac{1}{\\widehat{\\sigma^2}_{MLE}} + \\frac{1}{2} \\frac{1}{(\\widehat{\\sigma^2}_{MLE})^2}\\sum_{i=1}^{n}(\\ln y_i - \\widehat{\\mu}_{MLE})^2 = 0$$\n",
    "\n",
    "From which, we get the following estimates (provided that $\\sigma^2 \\neq 0$),  \n",
    "$$ \\widehat{\\mu}_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}\\ln y_i \\\\\n",
    " \\widehat{\\sigma^2}_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}(\\ln y_i-\\widehat{\\mu}_{MLE})^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f411ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_lowest = np.inner(np.array(aggregated_dataset['weight']), np.array(aggregated_dataset['log_income_amount']))\n",
    "mu_mean = np.inner(np.array(aggregated_dataset['weight']), np.array(aggregated_dataset['log_income_mean']))\n",
    "mu_highest = np.inner(np.array(aggregated_dataset['weight']), np.array(aggregated_dataset['log_income_max']))\n",
    "\n",
    "sigma_sq_lowest = np.inner(np.array(aggregated_dataset['weight']), (np.array(aggregated_dataset['log_income_amount'])-mu_lowest)**2)\n",
    "sigma_sq_mean = np.inner(np.array(aggregated_dataset['weight']), (np.array(aggregated_dataset['log_income_mean'])-mu_mean)**2)\n",
    "sigma_sq_highest = np.inner(np.array(aggregated_dataset['weight']), (np.array(aggregated_dataset['log_income_max'])-mu_highest)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "74d6f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_result = df({'assumption': [\"a) Lowest\", \"b) Mean\", \"c) Highest\"], \n",
    "                'mu_hat': [mu_lowest, mu_mean, mu_highest],\n",
    "                'sigma_sq_hat': [sigma_sq_lowest, sigma_sq_mean, sigma_sq_highest]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "af0a34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE_mu = mu_highest\n",
    "MLE_sigma = sigma_sq_highest ** (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "69a0f8c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assumption</th>\n",
       "      <th>mu_hat</th>\n",
       "      <th>sigma_sq_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a) Lowest</td>\n",
       "      <td>10.795785</td>\n",
       "      <td>2.239736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b) Mean</td>\n",
       "      <td>11.044071</td>\n",
       "      <td>0.871274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c) Highest</td>\n",
       "      <td>11.200075</td>\n",
       "      <td>0.967943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   assumption     mu_hat  sigma_sq_hat\n",
       "0   a) Lowest  10.795785      2.239736\n",
       "1     b) Mean  11.044071      0.871274\n",
       "2  c) Highest  11.200075      0.967943"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e708d2c",
   "metadata": {},
   "source": [
    "## Q.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2a630",
   "metadata": {},
   "source": [
    "We can estimate the standard error of each moment using sample hessian.  \n",
    "Hessian matrix equals the second derivative matrix of the log-likelihood function.  \n",
    "I calculated the hessian matrix as follows:\n",
    "$$\\mathcal{H}(\\hat{\\mu}, \\hat{\\sigma}) = \\begin{bmatrix}\n",
    "\\frac{1}{\\hat{\\sigma}^2} & \\frac{1}{\\hat{\\sigma}^4}\\frac{1}{n}\\sum_i^n (\\ln y_i - \\hat{\\mu}) \\\\\n",
    "\\frac{1}{\\hat{\\sigma}^4}\\frac{1}{n}\\sum_i^n (\\ln y_i - \\hat{\\mu}) & -\\frac{1}{2\\hat{\\sigma}^4} + \\frac{1}{\\hat{\\sigma}^6}\\frac{1}{n}\\sum_i^n (\\ln y_i - \\hat{\\mu})^2\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ef7a2d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated standard error of MoM mean esimator:  0.9838409787672148\n",
      "estimated standard error of MoM variance esimator:  1.0370577249618638\n"
     ]
    }
   ],
   "source": [
    "H_11 = 1/sigma_sq_highest\n",
    "H_12 = (1/(sigma_sq_highest)**2)*(np.inner(np.array(aggregated_dataset['weight']), (np.array(aggregated_dataset['log_income_max'])-mu_highest))) \n",
    "H_21 = (1/(sigma_sq_highest)**2)*(np.inner(np.array(aggregated_dataset['weight']), (np.array(aggregated_dataset['log_income_max'])-mu_highest))) \n",
    "H_22 = -(1/(2*(sigma_sq_highest)**2)) + (1/(sigma_sq_mean)**3)*np.inner(np.array(aggregated_dataset['weight']), (np.array(aggregated_dataset['log_income_max'])-mu_highest)**2)\n",
    "\n",
    "Hessian = np.array([\n",
    "    [H_11, H_12],\n",
    "    [H_21, H_22]\n",
    "    ])\n",
    "\n",
    "Hessian_inv = np.linalg.inv(Hessian) \n",
    "\n",
    "print(\"estimated standard error of MoM mean esimator: \", (Hessian_inv[0][0] ** (1/2)))\n",
    "print(\"estimated standard error of MoM variance esimator: \", Hessian_inv[1][1] ** (1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce4cd0",
   "metadata": {},
   "source": [
    "## Q.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8da55",
   "metadata": {},
   "source": [
    "It turns out that the point estimate depends on the assumptions for income bins. Especially, when we assumed the lowest income for each bin, estimate for the mean decreased by far compared to the other assumptions. Also, estimate for the variance increased. This is because under the log transformation, bin3's income level will vanish to zero. It makes the average level of income decrease and at the same time elevates the variance.  \n",
    "\n",
    "According to the Cramer-Rao Efficiency, MLE estimator should be more efficient than the MoM estimator. However, my result turns shows the opposite.  \n",
    "<span style = \"color:red\"> (I should go over this part.. I regarded the projection factor telling us the number of people in each bin. So the N is the total sum of the projection factor. Maybe this understanding could be incorrect. I will go over this part)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba4e05",
   "metadata": {},
   "source": [
    "## Q. 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ff4cb",
   "metadata": {},
   "source": [
    "Using the data, we can construct a sample analogy for the vector of moment equations as follows:  \n",
    "\n",
    "$$\\bar{g}(\\mu, \\sigma) = \\begin{bmatrix} \n",
    "Pr(\\ln y_i\\in bin_3|\\mu, \\sigma) - w_3 \\\\\n",
    "Pr(\\ln y_i\\in bin_4|\\mu, \\sigma) - w_4 \\\\\n",
    "Pr(\\ln y_i\\in bin_6|\\mu, \\sigma) - w_6 \\\\ \n",
    "\\cdots \\\\\n",
    "Pr(\\ln y_i\\in bin_{29}|\\mu, \\sigma) - w_{29} \\\\ \n",
    "Pr(\\ln y_i\\in bin_{30}|\\mu, \\sigma) - w_{30} \\\\ \n",
    "\\end{bmatrix}$$  \n",
    "where $\\ln y_i \\sim N(\\mu, \\sigma)$ and $w_b$ stands for the projection weight of $bin_b$. Hence we can write the Jacobian of the moment conditions as follows:  \n",
    "\n",
    "$$\\mathcal{J}(\\mu, \\sigma) = \\mathbb{E}\\begin{bmatrix} \n",
    "\\frac{\\partial Pr(\\ln y_i\\in bin_3|\\mu, \\sigma) - w_3}{\\partial \\mu} & \\frac{\\partial Pr(\\ln y_i\\in bin_3|\\mu, \\sigma) - w_3}{\\partial \\sigma}\\\\\n",
    "\\frac{\\partial Pr(\\ln y_i\\in bin_4|\\mu, \\sigma) - w_4}{\\partial \\mu} & \\frac{\\partial Pr(\\ln y_i\\in bin_4|\\mu, \\sigma) - w_4}{\\partial \\sigma}\\\\\n",
    "\\frac{\\partial Pr(\\ln y_i\\in bin_6|\\mu, \\sigma) - w_6}{\\partial \\mu} & \\frac{\\partial Pr(\\ln y_i\\in bin_6|\\mu, \\sigma) - w_6}{\\partial \\sigma}\\\\ \n",
    "\\cdots & \\cdots\\\\\n",
    "\\frac{\\partial Pr(\\ln y_i\\in bin_{29}|\\mu, \\sigma) - w_{29}}{\\partial \\mu} & \\frac{\\partial Pr(\\ln y_i\\in bin_{29}|\\mu, \\sigma) - w_{29}}{\\partial \\sigma}\\\\ \n",
    "\\frac{\\partial Pr(\\ln y_i\\in bin_{30}|\\mu, \\sigma) - w_{30}}{\\partial \\mu} & \\frac{\\partial Pr(\\ln y_i\\in bin_{30}|\\mu, \\sigma) - w_{30}}{\\partial \\sigma}\\\\ \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a58f7",
   "metadata": {},
   "source": [
    "## Q. 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217defc6",
   "metadata": {},
   "source": [
    "### First Stage Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0a879a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_income_list = list(aggregated_dataset['log_income_amount'])\n",
    "weight_list = list(aggregated_dataset['weight'])\n",
    "\n",
    "def gmm_criterion(parameter):\n",
    "    mu = parameter[0]\n",
    "    sigma = parameter[1]\n",
    "    benchmark_norm = scipy.stats.norm(mu, sigma)\n",
    "    \n",
    "    g_n_bar = []\n",
    "    for i in range(len(log_income_list)):\n",
    "        if i != len(log_income_list)-1:\n",
    "            g_n_bar.append(benchmark_norm.cdf(log_income_list[i+1]) - benchmark_norm.cdf(log_income_list[i]) - weight_list[i])\n",
    "        else:\n",
    "            g_n_bar.append(benchmark_norm.cdf(100) - benchmark_norm.cdf(log_income_list[i]) - weight_list[i])\n",
    "    \n",
    "    g_n_bar = np.array(g_n_bar)\n",
    "    criterion = g_n_bar @ g_n_bar\n",
    "    \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a331245b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003787\n",
      "         Iterations: 67\n",
      "         Function evaluations: 132\n"
     ]
    }
   ],
   "source": [
    "sample_init = np.array([10, 1.5])\n",
    "res = minimize(gmm_criterion, sample_init, method='nelder-mead',\n",
    "               options={'xatol': 1e-8, 'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5cc93b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One step GMM mean estimate:  11.144447971538549\n",
      "One step GMM variance estimate:  0.655830154115503\n"
     ]
    }
   ],
   "source": [
    "print(\"One step GMM mean estimate: \", res.x[0])\n",
    "print(\"One step GMM variance estimate: \", res.x[1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91838c47",
   "metadata": {},
   "source": [
    "### Second Stage Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c379796c",
   "metadata": {},
   "source": [
    "#### calculate efficient weight matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85f6f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_vector = []\n",
    "mu = res.x[0]\n",
    "sigma = res.x[1]\n",
    "benchmark_norm = scipy.stats.norm(mu, sigma) \n",
    "\n",
    "error_vector = []\n",
    "for i in range(len(log_income_list)):\n",
    "    if i != len(log_income_list)-1:\n",
    "        error_vector.append(benchmark_norm.cdf(log_income_list[i+1]) - benchmark_norm.cdf(log_income_list[i]) - weight_list[i])\n",
    "    else:\n",
    "        error_vector.append(benchmark_norm.cdf(100) - benchmark_norm.cdf(log_income_list[i]) - weight_list[i])\n",
    "\n",
    "error_vector = np.array(error_vector)\n",
    "ee = np.outer(error_vector, error_vector) / np.array(aggregated_dataset['projection_total']).sum()\n",
    "W_hat = np.linalg.inv(ee) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0763cfa1",
   "metadata": {},
   "source": [
    "#### 2 step GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30073cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_criterion2(parameter):\n",
    "    mu = parameter[0]\n",
    "    sigma = parameter[1]\n",
    "    benchmark_norm = scipy.stats.norm(mu, sigma)\n",
    "    \n",
    "    g_n_bar = []\n",
    "    for i in range(len(log_income_list)):\n",
    "        if i != len(log_income_list)-1:\n",
    "            g_n_bar.append(benchmark_norm.cdf(log_income_list[i+1]) - benchmark_norm.cdf(log_income_list[i]) - weight_list[i])\n",
    "        else:\n",
    "            g_n_bar.append(benchmark_norm.cdf(100) - benchmark_norm.cdf(log_income_list[i]) - weight_list[i])\n",
    "    \n",
    "    g_n_bar = np.array(g_n_bar)\n",
    "    criterion = g_n_bar @ W_hat @ g_n_bar\n",
    "    \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97ef4085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003787\n",
      "         Iterations: 67\n",
      "         Function evaluations: 132\n"
     ]
    }
   ],
   "source": [
    "res2 = minimize(gmm_criterion, sample_init, method='nelder-mead',\n",
    "               options={'xatol': 1e-8, 'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b39f231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two step GMM mean estimate:  11.144447971538549\n",
      "Two step GMM variance estimate:  0.655830154115503\n"
     ]
    }
   ],
   "source": [
    "print(\"Two step GMM mean estimate: \", res2.x[0])\n",
    "print(\"Two step GMM variance estimate: \", res2.x[1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f31b9",
   "metadata": {},
   "source": [
    "### Calculate Standard Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b679a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian = []\n",
    "for i in range(len(log_income_list)):\n",
    "\n",
    "    def derivative_for_mu(mu):\n",
    "        benchmark_norm = scipy.stats.norm(mu, 0.80983341)\n",
    "        if i != len(log_income_list)-1:\n",
    "            value = benchmark_norm.cdf(log_income_list[i+1]) - benchmark_norm.cdf(log_income_list[i])\n",
    "        else:\n",
    "            value = benchmark_norm.cdf(100) - benchmark_norm.cdf(log_income_list[i])\n",
    "        return value\n",
    "\n",
    "    def derivative_for_sigma(sigma):\n",
    "        benchmark_norm = scipy.stats.norm(11.14444796, sigma)\n",
    "        if i != len(log_income_list)-1:\n",
    "            value = benchmark_norm.cdf(log_income_list[i+1]) - benchmark_norm.cdf(log_income_list[i])\n",
    "        else:\n",
    "            value = benchmark_norm.cdf(100) - benchmark_norm.cdf(log_income_list[i])\n",
    "        return value\n",
    "    \n",
    "    der_mu = derivative(derivative_for_mu, 11.14444796, dx = 1e-6)\n",
    "    der_sigma = derivative(derivative_for_sigma, 0.80983341, dx = 1e-6)\n",
    "    \n",
    "    jacobian.append([der_mu, der_sigma])\n",
    "\n",
    "jacobian = np.array(jacobian)\n",
    "cov_matrix = jacobian.transpose() @ W_hat @ jacobian\n",
    "V_hat = -np.linalg.inv(cov_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b4fe86ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated standard error of GMM mean esimator:  1.1698346781362853e-12\n",
      "estimated standard error of GMM variance esimator:  5.057059257142951e-13\n"
     ]
    }
   ],
   "source": [
    "print(\"estimated standard error of GMM mean esimator: \", V_hat[0][0] ** (1/2))\n",
    "print(\"estimated standard error of GMM variance esimator: \", V_hat[1][1] ** (1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044b8618",
   "metadata": {},
   "source": [
    "## Q.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "597bce4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>𝜇̂</th>\n",
       "      <th>𝜎̂</th>\n",
       "      <th>mean income</th>\n",
       "      <th>p10 income</th>\n",
       "      <th>p90 income</th>\n",
       "      <th>std error of 𝜇̂</th>\n",
       "      <th>std error of 𝜎̂</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MoM(Lowest Ass)</td>\n",
       "      <td>10.795785</td>\n",
       "      <td>1.496575</td>\n",
       "      <td>48814.598532</td>\n",
       "      <td>7171.334326</td>\n",
       "      <td>332276.382840</td>\n",
       "      <td>8.033272e-04</td>\n",
       "      <td>1.700223e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLE (Highest Ass)</td>\n",
       "      <td>11.200075</td>\n",
       "      <td>0.983841</td>\n",
       "      <td>73135.951148</td>\n",
       "      <td>20727.827208</td>\n",
       "      <td>258052.486485</td>\n",
       "      <td>9.838410e-01</td>\n",
       "      <td>1.037058e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GMM</td>\n",
       "      <td>11.144448</td>\n",
       "      <td>0.809833</td>\n",
       "      <td>69178.677917</td>\n",
       "      <td>24504.319703</td>\n",
       "      <td>195299.830246</td>\n",
       "      <td>1.169835e-12</td>\n",
       "      <td>5.057059e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Method         𝜇̂        𝜎̂   mean income    p10 income  \\\n",
       "0    MoM(Lowest Ass)  10.795785  1.496575  48814.598532   7171.334326   \n",
       "1  MLE (Highest Ass)  11.200075  0.983841  73135.951148  20727.827208   \n",
       "2                GMM  11.144448  0.809833  69178.677917  24504.319703   \n",
       "\n",
       "      p90 income  std error of 𝜇̂  std error of 𝜎̂  \n",
       "0  332276.382840     8.033272e-04     1.700223e-03  \n",
       "1  258052.486485     9.838410e-01     1.037058e+00  \n",
       "2  195299.830246     1.169835e-12     5.057059e-13  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method = ['MoM(Lowest Ass)', 'MLE (Highest Ass)', 'GMM']\n",
    "mu =[MoM_mu, MLE_mu, res.x[0]]\n",
    "sigma = [MoM_sigma, MLE_sigma, res.x[1]]\n",
    "mean = [np.exp(MoM_mu), np.exp(MLE_mu), np.exp(res.x[0])]\n",
    "p10 = [np.exp(scipy.stats.norm(MoM_mu, MoM_sigma).ppf(0.1)), np.exp(scipy.stats.norm(MLE_mu, MLE_sigma).ppf(0.1)), np.exp(scipy.stats.norm(res.x[0], res.x[1]).ppf(0.1))]\n",
    "p90 = [np.exp(scipy.stats.norm(MoM_mu, MoM_sigma).ppf(0.9)), np.exp(scipy.stats.norm(MLE_mu, MLE_sigma).ppf(0.9)), np.exp(scipy.stats.norm(res.x[0], res.x[1]).ppf(0.9))]\n",
    "std_err_mu = [MoM_std_err_mean, Hessian_inv[0][0] ** (1/2), V_hat[0][0] ** (1/2)]\n",
    "std_err_sigma = [MoM_std_err_var, Hessian_inv[1][1] ** (1/2), V_hat[1][1] ** (1/2)]\n",
    "\n",
    "result_table = df({'Method':method,\n",
    "                  '𝜇̂':mu,\n",
    "                  '𝜎̂':sigma,\n",
    "                  'mean income': mean,\n",
    "                  'p10 income': p10,\n",
    "                  'p90 income': p90,\n",
    "                  'std error of 𝜇̂': std_err_mu,\n",
    "                  'std error of 𝜎̂': std_err_sigma})\n",
    "\n",
    "result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf84e9c",
   "metadata": {},
   "source": [
    "I reported the result of MoM under the lowest income assumption. And for the MLE result I followed the highest income assumption.  \n",
    "\n",
    "Differences in the estimated parameters get amplified by transforming it into the dollor unit. From the different value in mean income level between MoM and MLE, we can check that the assumption for bin typed data makes a significant differences.  \n",
    "\n",
    "As we can check from the column 'mean income', choosing lowest level in each bin can underestimate the income status whereas choosing the highest income can lead to an inflated result.  \n",
    "\n",
    "When working with the bin typed data, I would rather take the <b>GMM approach</b>.\n",
    "While constructing the moment equations, we did not have to set up any ridiculous assumption with respect to the income bin. We only deployed the distributional information, which could be more free from underestimating/overesimating issues caused by naive assumptions.  \n",
    "\n",
    "<span style = \"color:red\"> Regarding the variance of each estimator, it really seems that I did not fully understand the data structure. I will go over this part again.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
